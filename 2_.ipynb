{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Emotion_final.csv') ## import du dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i\" \"feel\" \"and\" \"to\" \"the\" \"a\" \"of\" \"that\" \"feeling\" \"my\" \"in\" \"it\" \"like\" \"was\" \"so\" \"for\" \"im\" \"me\" \"but\" \"have\" \"is\" \"with\" \"this\" \"am\" \"not\" \"about\" \"be\" \"as\" \"on\" \"you'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "all_text = ' '.join(df.Text)\n",
    "words = all_text.split()\n",
    "word_counts = Counter(words)\n",
    "'\" \"'.join([word for word, count in word_counts.most_common(30)]) \n",
    "## vvv Listes des 30 mots les plus courants du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ^ Liste des 30 mots les plus courants du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestion des stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/apprenant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_filter = ['feel', 'feeling', 'like', ',', 'really', 'know', 'get', 'would', 'time', 'little', 'ive', 'still', 'even', 'want', 'life', 'way', 'could', 'back', 'make', 'time', 'going', 'know', 'im', 'one', 'bit', 'much', 'dont', 'day', 'think', 'one', 'always', 'people', 'things', 'something', 'today', 'go', 'see', 'work', 'cant', 'say', 'never', 'didnt', 'made', 'someone', 'many', 'pretty', 'right', 'felt', 'feelings', 'though', 'also', 'need', 'every', 'lot', 'around', \"'s\", 'look', 'every', 'new', 'year', 'able', 'got', 'also', 'less', 'feels', 'home', 'last', 'days', 'come', 'actually', 'makes']\n",
    "## Liste des mots qui ne sont pas dans les stopwords de NLTK mais que l'on souhaite tout de même retirer pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_emotion = dict()\n",
    "for emotion in df.Emotion.unique():  # Pour chaque émotion on va vérifier quels mots sont les plus fréquents en dehors des stopwords\n",
    "    \n",
    "    df_temp = df[df.Emotion == emotion]     # Filtre le dataset sur l'emotion itérée\n",
    "    words = ' '.join(df_temp.Text).split()      # Mets tous les mots de la colonne text dans une liste\n",
    "    \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]     \n",
    "    filtered__words = [word for word in filtered_words if word.lower() not in manual_filter]    \n",
    "        # Retire tous les stopwords et le filtre écrit manuellement\n",
    "        \n",
    "    word_counts = Counter(filtered__words)  # Compte les mots par occurences\n",
    "    \n",
    "    words_by_emotion[emotion] = (\", \".join([word for word, count in word_counts.most_common(30)])).split(', ') \n",
    "    # mets les 30 premiers résultats dans le dictionnaire pour chaque emotion iterée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m  Sadness : ['love', 'sad', 'alone', 'bad', 'depressed', 'good', 'miserable', 'kind', 'lost', 'left', 'help', 'sorry', 'stupid', 'guilty', 'without', 'stressed', 'lonely', 'exhausted', 'hurt', 'said', 'friends', 'ashamed', 'devastated', 'away', 'sometimes', 'low', 'pain', 'week', 'punished', 'http'] \n",
      " \u001b[91m Anger : ['angry', 'offended', 'resentful', 'cold', 'irritable', 'bothered', 'greedy', 'mad', 'insulted', 'irritated', 'pissed', 'violent', 'annoyed', 'hated', 'dissatisfied', 'fucked', 'rude', 'bitchy', 'cranky', 'frustrated', 'rushed', 'dangerous', 'stressed', 'selfish', 'bitter', 'disgusted', 'distracted', 'agitated', 'jealous', 'love'] \n",
      " \u001b[95m Love : ['love', 'sweet', 'loving', 'caring', 'passionate', 'sympathetic', 'liked', 'hot', 'tender', 'lovely', 'longing', 'loved', 'accepted', 'nostalgic', 'gentle', 'horny', 'naughty', 'romantic', 'blessed', 'supporting', 'loyal', 'supportive', 'beloved', 'fond', 'generous', 'delicate', 'faithful', 'towards', 'good', 'friends'] \n",
      " \u001b[90m Surprise : ['amazed', 'impressed', 'curious', 'overwhelmed', 'funny', 'surprised', 'weird', 'strange', 'amazing', 'shocked', 'stunned', 'dazed', 'remember', 'looked', 'bewildered', '`', 'find', 'startled', 'everything', 'love', 'trying', 'bewilderment', 'astonishment', 'left', 'started', 'world', 'help', 'sometimes', 'good', '\"'] \n",
      " \u001b[92m Fear : ['anxious', 'nervous', 'strange', 'terrified', 'afraid', 'agitated', 'scared', 'frightened', 'weird', 'unsure', 'vulnerable', 'apprehensive', 'uncertain', 'overwhelmed', 'shaken', 'pressured', 'hesitant', 'reluctant', 'paranoid', 'intimidated', 'shy', 'insecure', 'uncomfortable', 'shaky', 'helpless', 'threatened', 'love', 'fearful', 'restless', 'confused'] \n",
      " \u001b[93m Happy : ['good', 'love', 'happy', 'well', 'enough', 'sure', 'excited', 'quite', 'better', 'help', 'first', 'important', 'free', 'find', 'thankful', 'ever', 'glad', 'http', 'take', 'anything', 'cool', 'wonderful', 'person', 'confident', 'proud', 'part', 'special', 'everything', 'ok', 'family'] \n",
      " \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'\\033[94m  Sadness :', words_by_emotion['sadness'], '\\n',  '\\033[91m Anger :', words_by_emotion['anger'], '\\n',  '\\033[95m Love :', words_by_emotion['love'], '\\n',  '\\033[90m Surprise :', words_by_emotion['surprise'], '\\n',  '\\033[92m Fear :', words_by_emotion['fear'], '\\n',  '\\033[93m Happy :', words_by_emotion['happy'], '\\n',  '\\033[0m')\n",
    "## vvv Mots les plus fréquents pour chaque émotion hors stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion\n",
      "happy       7029\n",
      "sadness     6265\n",
      "anger       2993\n",
      "fear        2652\n",
      "love        1641\n",
      "surprise     879\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/apprenant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {\"needn't\", 'wasn', 'd', 'other', 'very', 'and', 'hers', \"mustn't\", 'shouldn', 'during', 'yourselves', 'an', 'same', 'had', 'yourself', 'down', 'where', 'they', 'them', \"shouldn't\", 'were', 'above', 'than', \"mightn't\", 'after', 'being', 'so', 'was', 'couldn', 'what', 'i', 'ours', 'weren', 'further', 'any', 'to', 'you', 'themselves', 'in', 'o', 'mightn', 'do', 'their', 'won', 's', 'why', \"aren't\", 'by', 'does', 'will', 'against', 'should', 'be', 'been', 'we', 'which', 'few', 'those', 'that', 'through', \"wouldn't\", \"should've\", 'more', 'with', \"isn't\", 'hasn', 'its', 'now', \"you've\", 'once', 'll', \"wasn't\", 'nor', 'how', 'y', \"you'd\", 'off', \"weren't\", 'doesn', 'there', 'ma', 'again', 'theirs', 'our', 'before', 'until', 're', 'her', 'of', 'both', \"hadn't\", 'out', 'most', 've', 'don', 'all', 'hadn', \"doesn't\", 'such', 'too', 'whom', 'no', 'can', 'mustn', 'own', 'herself', 'ourselves', 'each', 'wouldn', 'isn', \"don't\", 'has', 'over', 'himself', 'under', 'she', 'while', 'him', 'about', \"shan't\", 'for', 'doing', 'your', 'myself', \"hasn't\", 'is', 'if', 'ain', 'haven', 'not', 'when', 'have', 'here', 'or', 'from', 'because', 'needn', 'but', \"you're\", \"it's\", 'it', 'just', 'as', 'below', 'having', 'the', 'between', 'me', 'shan', 'then', 'at', 'a', 'his', 'these', \"haven't\", 'yours', \"won't\", 'am', 'into', 't', \"couldn't\", 'aren', 'itself', 'my', \"you'll\", \"she's\", 'didn', 'up', 'are', 'did', \"that'll\", 'some', 'only', 'he', 'm', 'on', 'this', \"didn't\", 'who'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m texts \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mEmotion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m emotion][\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(stop_words\u001b[39m=\u001b[39mstop_words)\n\u001b[0;32m---> 21\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(texts)\n\u001b[1;32m     22\u001b[0m word_counts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(vectorizer\u001b[39m.\u001b[39mget_feature_names(), X\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     23\u001b[0m word_counts \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(word_counts, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Projets/NLP/psycho_NLP/venv/lib/python3.10/site-packages/sklearn/base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m partial_fit_and_fitted \u001b[39m=\u001b[39m (\n\u001b[1;32m   1140\u001b[0m     fit_method\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpartial_fit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1141\u001b[0m )\n\u001b[1;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m global_skip_validation \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1144\u001b[0m     estimator\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[1;32m   1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Projets/NLP/psycho_NLP/venv/lib/python3.10/site-packages/sklearn/base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    630\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \n\u001b[1;32m    632\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[1;32m    639\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    640\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[1;32m    641\u001b[0m     )\n",
      "File \u001b[0;32m~/Projets/NLP/psycho_NLP/venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {\"needn't\", 'wasn', 'd', 'other', 'very', 'and', 'hers', \"mustn't\", 'shouldn', 'during', 'yourselves', 'an', 'same', 'had', 'yourself', 'down', 'where', 'they', 'them', \"shouldn't\", 'were', 'above', 'than', \"mightn't\", 'after', 'being', 'so', 'was', 'couldn', 'what', 'i', 'ours', 'weren', 'further', 'any', 'to', 'you', 'themselves', 'in', 'o', 'mightn', 'do', 'their', 'won', 's', 'why', \"aren't\", 'by', 'does', 'will', 'against', 'should', 'be', 'been', 'we', 'which', 'few', 'those', 'that', 'through', \"wouldn't\", \"should've\", 'more', 'with', \"isn't\", 'hasn', 'its', 'now', \"you've\", 'once', 'll', \"wasn't\", 'nor', 'how', 'y', \"you'd\", 'off', \"weren't\", 'doesn', 'there', 'ma', 'again', 'theirs', 'our', 'before', 'until', 're', 'her', 'of', 'both', \"hadn't\", 'out', 'most', 've', 'don', 'all', 'hadn', \"doesn't\", 'such', 'too', 'whom', 'no', 'can', 'mustn', 'own', 'herself', 'ourselves', 'each', 'wouldn', 'isn', \"don't\", 'has', 'over', 'himself', 'under', 'she', 'while', 'him', 'about', \"shan't\", 'for', 'doing', 'your', 'myself', \"hasn't\", 'is', 'if', 'ain', 'haven', 'not', 'when', 'have', 'here', 'or', 'from', 'because', 'needn', 'but', \"you're\", \"it's\", 'it', 'just', 'as', 'below', 'having', 'the', 'between', 'me', 'shan', 'then', 'at', 'a', 'his', 'these', \"haven't\", 'yours', \"won't\", 'am', 'into', 't', \"couldn't\", 'aren', 'itself', 'my', \"you'll\", \"she's\", 'didn', 'up', 'are', 'did', \"that'll\", 'some', 'only', 'he', 'm', 'on', 'this', \"didn't\", 'who'} instead."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "emotions_counts = df.Emotion.value_counts()\n",
    "\n",
    "##############################################\n",
    "# Pour chaque sentiment, identifiez les 30 mots les plus courants en dehors des stopwords\n",
    "words_by_emotion = {}\n",
    "for emotion in df.Emotion.unique():\n",
    "    texts = df[df.Emotion == emotion]['Text']\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    word_counts = list(zip(vectorizer.get_feature_names(), X.sum(axis=0).tolist()[0]))\n",
    "    word_counts = sorted(word_counts, key=lambda x: x[1], reverse=True)\n",
    "    top_words = [word for word, count in word_counts if word not in stop_words][:30]\n",
    "    words_by_emotion[emotion] = top_words\n",
    "\n",
    "# Définissez une matrice de similarité entre les sentiments\n",
    "sentiments = df['Emotion'].unique()\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Affichez la matrice de similarité sous forme de heatmap\n",
    "sns.heatmap(similarity_matrix, xticklabels=sentiments, yticklabels=sentiments, cmap='coolwarm', annot=True)\n",
    "plt.xlabel('Sentiments')\n",
    "plt.ylabel('Sentiments')\n",
    "plt.title('Similarité entre les sentiments')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
